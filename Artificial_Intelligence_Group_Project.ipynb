{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lee38667/Artificial-Intelligence-Group-Project/blob/main/Artificial_Intelligence_Group_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywImSgFmLJwq"
      },
      "source": [
        "**PART 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBEn1m-xTZ-7"
      },
      "source": [
        "**Loading the CSV into Dataframes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZI_LOnBx3MH"
      },
      "source": [
        "**Co author comparison graph**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "91CMoLhGx9rd",
        "outputId": "64715a67-0bbc-411f-c065-b40de552178e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "Usage: python Scientist.py [directory]",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Usage: python Scientist.py [directory]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import sys\n",
        "from collections import deque\n",
        "\n",
        "# Data structures\n",
        "names = {}\n",
        "scientists = {}\n",
        "papers = {}\n",
        "paper_to_scientists = {}\n",
        "scientist_to_papers = {}\n",
        "\n",
        "def load_data(directory):\n",
        "    # Load scientists\n",
        "    with open(f\"{directory}/scientists.csv\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            scientists[row[\"scientist_id\"]] = {\n",
        "                \"name\": row[\"name\"],\n",
        "                \"papers\": set()\n",
        "            }\n",
        "            if row[\"name\"] not in names:\n",
        "                names[row[\"name\"]] = {row[\"scientist_id\"]}\n",
        "            else:\n",
        "                names[row[\"name\"]].add(row[\"scientist_id\"])\n",
        "\n",
        "    # Load papers\n",
        "    with open(f\"{directory}/papers.csv\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            papers[row[\"paper_id\"]] = {\n",
        "                \"title\": row[\"title\"],\n",
        "                \"year\": row[\"year\"],\n",
        "                \"authors\": set()\n",
        "            }\n",
        "\n",
        "    # Load authors\n",
        "    with open(f\"{directory}/authors.csv\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            try:\n",
        "                scientist_id = row[\"scientist_id\"]\n",
        "                paper_id = row[\"paper_id\"]\n",
        "                scientists[scientist_id][\"papers\"].add(paper_id)\n",
        "                papers[paper_id][\"authors\"].add(scientist_id)\n",
        "\n",
        "                # Build mappings\n",
        "                if paper_id not in paper_to_scientists:\n",
        "                    paper_to_scientists[paper_id] = set()\n",
        "                paper_to_scientists[paper_id].add(scientist_id)\n",
        "\n",
        "                if scientist_id not in scientist_to_papers:\n",
        "                    scientist_to_papers[scientist_id] = set()\n",
        "                scientist_to_papers[scientist_id].add(paper_id)\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "def neighbors_for_person(scientist_id):\n",
        "    neighbors = set()\n",
        "    for paper_id in scientist_to_papers.get(scientist_id, set()):\n",
        "        for coauthor_id in paper_to_scientists.get(paper_id, set()):\n",
        "            if coauthor_id != scientist_id:\n",
        "                neighbors.add((paper_id, coauthor_id))\n",
        "    return neighbors\n",
        "\n",
        "def shortest_path(source, target):\n",
        "    if source == target:\n",
        "        return []\n",
        "\n",
        "    queue = deque()\n",
        "    queue.append((source, []))\n",
        "    visited = set()\n",
        "    visited.add(source)\n",
        "\n",
        "    while queue:\n",
        "        current, path = queue.popleft()\n",
        "        for paper_id, neighbor in neighbors_for_person(current):\n",
        "            if neighbor == target:\n",
        "                return path + [(paper_id, neighbor)]\n",
        "            if neighbor not in visited:\n",
        "                visited.add(neighbor)\n",
        "                queue.append((neighbor, path + [(paper_id, neighbor)]))\n",
        "    return None\n",
        "\n",
        "def person_id_for_name(name):\n",
        "    person_ids = list(names.get(name, set()))\n",
        "    if not person_ids:\n",
        "        return None\n",
        "    elif len(person_ids) > 1:\n",
        "        print(f\"Which '{name}'?\")\n",
        "        for pid in person_ids:\n",
        "            print(f\"ID: {pid}, Name: {scientists[pid]['name']}\")\n",
        "        return input(\"Intended ID: \").strip()\n",
        "    else:\n",
        "        return person_ids[0]\n",
        "\n",
        "def main():\n",
        "    if len(sys.argv) != 2:\n",
        "        sys.exit(\"Usage: python Scientist.py [directory]\")\n",
        "    directory = sys.argv[1]\n",
        "\n",
        "    print(\"Loading data...\")\n",
        "    load_data(directory)\n",
        "    print(\"Data loaded.\")\n",
        "\n",
        "    source_name = input(\"Name: \").strip()\n",
        "    source_id = person_id_for_name(source_name)\n",
        "    if source_id is None:\n",
        "        sys.exit(\"Scientist not found.\")\n",
        "\n",
        "    target_name = input(\"Name: \").strip()\n",
        "    target_id = person_id_for_name(target_name)\n",
        "    if target_id is None:\n",
        "        sys.exit(\"Scientist not found.\")\n",
        "\n",
        "    path = shortest_path(source_id, target_id)\n",
        "\n",
        "    if path is None:\n",
        "        print(\"No connection found.\")\n",
        "    else:\n",
        "        degrees = len(path)\n",
        "        print(f\"{degrees} degrees of separation.\")\n",
        "        current_id = source_id\n",
        "        for i, (paper_id, scientist_id) in enumerate(path, 1):\n",
        "            paper = papers[paper_id]\n",
        "            next_name = scientists[scientist_id][\"name\"]\n",
        "            current_name = scientists[current_id][\"name\"]\n",
        "            print(f\"{i}: {current_name} and {next_name} co-authored \\\"{paper['title']}\\\"\")\n",
        "            current_id = scientist_id\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZGAKylEZhlW"
      },
      "source": [
        "**Part 2 AI Sudoku Puzzle Solver**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1y_3TpnZsWR",
        "outputId": "1db34471-f322-4359-f162-a4cec79e0a39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution found:\n",
            "5 3 4 | 6 7 8 | 9 1 2 \n",
            "6 7 2 | 1 9 5 | 3 4 8 \n",
            "1 9 8 | 3 4 2 | 5 6 7 \n",
            "---------------------\n",
            "8 5 9 | 7 6 1 | 4 2 3 \n",
            "4 2 6 | 8 5 3 | 7 9 1 \n",
            "7 1 3 | 9 2 4 | 8 5 6 \n",
            "---------------------\n",
            "9 6 1 | 5 3 7 | 2 8 4 \n",
            "2 8 7 | 4 1 9 | 6 3 5 \n",
            "3 4 5 | 2 8 6 | 1 7 9 \n"
          ]
        }
      ],
      "source": [
        "class Sudoku_AI_Solver:\n",
        "    def __init__(self):\n",
        "        self.domains = {\n",
        "            (i, j): set(range(1, 10)) for i in range(9) for j in range(9)\n",
        "        }\n",
        "\n",
        "    def load_from_file(self, filename):\n",
        "        \"\"\"Load Sudoku puzzle from file with error handling to ensure clear instructions\"\"\"\n",
        "        try:\n",
        "            with open(filename, 'r') as f:\n",
        "                lines = [line.strip().replace(\" \", \"\") for line in f if line.strip()]\n",
        "\n",
        "            if len(lines) != 9:\n",
        "              #ensure there's no empty spaces even though we've added line correction (first issue encountered)\n",
        "                raise ValueError(\"File must contain exactly 9 lines\")\n",
        "\n",
        "            for i in range(9):\n",
        "                if len(lines[i]) != 9:\n",
        "                    raise ValueError(f\"Line {i+1} must contain exactly 9 characters\")\n",
        "                for j in range(9):\n",
        "                    val = int(lines[i][j])\n",
        "                    if val == 0:\n",
        "                        self.domains[(i, j)] = set(range(1, 10))\n",
        "                    elif 1 <= val <= 9:\n",
        "                        self.domains[(i, j)] = {val}\n",
        "                    else:\n",
        "                        raise ValueError(f\"Invalid value {val} at position ({i}, {j})\")\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"File {filename} not found\")\n",
        "        except ValueError as e:\n",
        "            raise ValueError(f\"Invalid Sudoku file: {str(e)}\")\n",
        "\n",
        "    def enforce_node_consistency(self):\n",
        "        for cell in self.domains:\n",
        "            if len(self.domains[cell]) == 1:\n",
        "                val = next(iter(self.domains[cell]))\n",
        "                self.domains[cell] = {val}\n",
        "\n",
        "    def revise(self, x, y):\n",
        "        revised = False\n",
        "        if len(self.domains[y]) == 1:\n",
        "            val_y = next(iter(self.domains[y]))\n",
        "            if val_y in self.domains[x]:\n",
        "                if len(self.domains[x]) > 1:\n",
        "                    self.domains[x].remove(val_y)\n",
        "                    revised = True\n",
        "        return revised\n",
        "\n",
        "    def ac3(self):\n",
        "        queue = [(x, y) for x in self.domains for y in self.get_neighbors(x)]\n",
        "        while queue:\n",
        "            x, y = queue.pop(0)\n",
        "            if self.revise(x, y):\n",
        "                if len(self.domains[x]) == 0:\n",
        "                    return False\n",
        "                for z in self.get_neighbors(x):\n",
        "                    if z != y:\n",
        "                        queue.append((z, x))\n",
        "        return True\n",
        "\n",
        "    def assignment_complete(self, assignment):\n",
        "        return all(len(assignment[cell]) == 1 for cell in assignment)\n",
        "\n",
        "    def consistent(self, assignment):\n",
        "        for cell in assignment:\n",
        "            if len(assignment[cell]) == 1:\n",
        "                val = next(iter(assignment[cell]))\n",
        "                for neighbor in self.get_neighbors(cell):\n",
        "                    if len(assignment[neighbor]) == 1:\n",
        "                        if val == next(iter(assignment[neighbor])):\n",
        "                            return False\n",
        "        return True\n",
        "\n",
        "    def order_domain_values(self, var, assignment):\n",
        "        def count_conflicts(value):\n",
        "            count = 0\n",
        "            for neighbor in self.get_neighbors(var):\n",
        "                if value in assignment[neighbor]:\n",
        "                    count += 1\n",
        "            return count\n",
        "        return sorted(assignment[var], key=count_conflicts)\n",
        "\n",
        "    def select_unassigned_variable(self, assignment):\n",
        "        unassigned = [v for v in assignment if len(assignment[v]) > 1]\n",
        "        return min(unassigned, key=lambda var: (len(assignment[var]), -len(self.get_neighbors(var))))\n",
        "\n",
        "    def backtrack(self, assignment):\n",
        "        if self.assignment_complete(assignment):\n",
        "            return assignment\n",
        "        var = self.select_unassigned_variable(assignment)\n",
        "        for value in self.order_domain_values(var, assignment):\n",
        "            new_assignment = {k: v.copy() for k, v in assignment.items()}\n",
        "            new_assignment[var] = {value}\n",
        "            if self.consistent(new_assignment):\n",
        "                result = self.backtrack(new_assignment)\n",
        "                if result:\n",
        "                    return result\n",
        "        return None\n",
        "\n",
        "    def get_neighbors(self, cell):\n",
        "        i, j = cell\n",
        "        neighbors = set()\n",
        "        for k in range(9):\n",
        "            if k != j:\n",
        "                neighbors.add((i, k))\n",
        "            if k != i:\n",
        "                neighbors.add((k, j))\n",
        "        top_left_i = 3 * (i // 3)\n",
        "        top_left_j = 3 * (j // 3)\n",
        "        for a in range(top_left_i, top_left_i + 3):\n",
        "            for b in range(top_left_j, top_left_j + 3):\n",
        "                if (a, b) != cell:\n",
        "                    neighbors.add((a, b))\n",
        "        return neighbors\n",
        "\n",
        "    def solve(self):\n",
        "        self.enforce_node_consistency()\n",
        "        if not self.ac3():\n",
        "            print(\"AC-3 failed\")\n",
        "            return None\n",
        "        return self.backtrack(self.domains)\n",
        "\n",
        "    def display(self, assignment):\n",
        "        for i in range(9):\n",
        "            if i % 3 == 0 and i != 0:\n",
        "                print(\"-\" * 21)\n",
        "            for j in range(9):\n",
        "                if j % 3 == 0 and j != 0:\n",
        "                    print(\"|\", end=\" \")\n",
        "                val = next(iter(assignment[(i, j)]))\n",
        "                print(val if val != 0 else \".\", end=\" \")\n",
        "            print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    solver = Sudoku_AI_Solver()\n",
        "    try:\n",
        "        solver.load_from_file(\"sudoku_easy.txt\")\n",
        "        solution = solver.solve()\n",
        "        if solution:\n",
        "            print(\"Solution found:\")\n",
        "            solver.display(solution)\n",
        "        else:\n",
        "            print(\"No solution found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part **3**"
      ],
      "metadata": {
        "id": "vJTPosoSISwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install neccesary libraries**"
      ],
      "metadata": {
        "id": "Z5JJeuzrhKIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow opencv-python scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-nNKnnYMhGbN",
        "outputId": "c6e5ad06-5f13-4357-ba96-a9a268dd4335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xCTB6iZfx0E"
      },
      "source": [
        "Part 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "EPOCHS = 50  # Let EarlyStopping decide the best epoch\n",
        "IMG_WIDTH = 30\n",
        "IMG_HEIGHT = 30\n",
        "NUM_CATEGORIES = 43\n",
        "TEST_SIZE = 0.4\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Check command-line arguments\n",
        "    if len(sys.argv) != 2:\n",
        "        sys.exit(\"Usage: python traffic.py data_directory\")\n",
        "\n",
        "    # Get image arrays and labels for all image files\n",
        "    images, labels = load_data(sys.argv[1])\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    labels = tf.keras.utils.to_categorical(labels)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "        np.array(images), np.array(labels), test_size=TEST_SIZE\n",
        "    )\n",
        "\n",
        "    # Get a compiled neural network\n",
        "    model = get_model()\n",
        "\n",
        "    # Early stopping callback\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    # Train the model with validation split and early stopping\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        epochs=EPOCHS,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[early_stop]\n",
        "    )\n",
        "\n",
        "    # Evaluate neural network performance\n",
        "    model.evaluate(x_test, y_test, verbose=2)\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(history)\n",
        "\n",
        "    # Save model with versioning\n",
        "    save_model_with_version(model)\n",
        "\n",
        "\n",
        "def load_data(data_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for label in range(NUM_CATEGORIES):\n",
        "        label_dir = os.path.join(data_dir, str(label))\n",
        "        if not os.path.isdir(label_dir):\n",
        "            continue\n",
        "\n",
        "        for filename in os.listdir(label_dir):\n",
        "            filepath = os.path.join(label_dir, filename)\n",
        "            image = cv2.imread(filepath)\n",
        "\n",
        "            if image is None:\n",
        "                continue\n",
        "\n",
        "            image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
        "            images.append(image)\n",
        "            labels.append(label)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        tf.keras.layers.Flatten(),\n",
        "\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "        tf.keras.layers.Dense(NUM_CATEGORIES, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def plot_training_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs_range = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training vs. Validation Accuracy')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training vs. Validation Loss')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def save_model_with_version(model, folder=\".\"):\n",
        "    \"\"\"\n",
        "    Save model to file as modelV#.h5, where # is the next available version number.\n",
        "    \"\"\"\n",
        "    version = 1\n",
        "    while os.path.exists(os.path.join(folder, f\"modelV{version}.h5\")):\n",
        "        version += 1\n",
        "\n",
        "    filename = os.path.join(folder, f\"modelV{version}.h5\")\n",
        "    model.save(filename)\n",
        "    print(f\"Model saved to {filename}.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "bNwqWgDuiIa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "1f4d61b5-f11f-4f7c-833b-0fcbafcd3b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "zero-size array to reduction operation maximum which has no identity",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-038b5acc6faf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-038b5acc6faf>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Split data into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     x_train, x_test, y_train, y_test = train_test_split(\n\u001b[1;32m     28\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTEST_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/numerical_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(x, num_classes)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36mmax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2897\u001b[0m     \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m     \"\"\"\n\u001b[0;32m-> 2899\u001b[0;31m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[1;32m   2900\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPCT7L8MQ50lluWdovhlkqc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}